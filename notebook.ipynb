{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [View Solution Notebook](./solution.html)\n",
    "- [View Project Page]()\n",
    "\n",
    "# Imports and Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f62b5146c1449e9be18f57aae8c2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\anush\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "259212aa9105493382a3c9bb46fc91b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ab82971f6442239af999912358063b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca79c7c39ef54decafe3a9ab8b4f5edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab9ba8fce9842f8b00e644ad2072d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b58cd611e8849f688a14693a1c0b219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92292970fdc4b4aaf32525d9f7d62cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Instantiate DistilGPT-2's `tokenizer` and `model` using the `.from_pretrained` method.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31373,    11,   995,     0]])\n"
     ]
    }
   ],
   "source": [
    "# 2. Assign pt_tensors the input text's tokens in PyTorch tensor form\n",
    "def encode_text_as_pt_tensor(text):\n",
    "    pt_tensors = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    return pt_tensors\n",
    "\n",
    "print(encode_text_as_pt_tensor(\"hello, world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "# 3. Use set_seed to make the rest of the notebook's output deterministic. Pass it the number 42.\n",
    "# set seed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[7120, 6152,  994,    0,  198,  198,  198,  198,  198,  198,  198,  198,\n",
       "          198,  198,  198,  198,  198,  198,  198,  198]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"Your prompt here!\"\n",
    "tokens = encode_text_as_pt_tensor(prompt)\n",
    "# 4. Instruct the model to generate a completion on your choice of prompt using the greedy search method.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id as the second argument to prevent seeing a warning.\n",
    "output_tokens = model.generate(tokens, pad_token_id = tokenizer.eos_token_id)\n",
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your prompt here!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Decode the resulting tokens.\n",
    "def decode_tokens(tokens):\n",
    "    text = tokenizer.decode(tokens)\n",
    "    return text\n",
    "\n",
    "decode_tokens(output_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting with Generation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discipline is what you hate to do but to do it like you love it.\"\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Discipline is what you hate to do but to do it like you love it.\"\\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Adapt the function below to use beam search in its generations. Then call it three times with 2 beam, 6 beams, and 14 beams.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "def generate_with_beam_search(prompt,num_beams):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens, num_beams=num_beams, pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "\n",
    "prompt = \"Discipline is what you hate to do but to do it like you love it\"\n",
    "generate_with_beam_search(prompt,14)\n",
    "# make multiple calls to the model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discipline is what you hate to do but to do it like you love it.”\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Discipline is what you hate to do but to do it like you love it.”\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Repeat the same process you did with beam search on step 6 with n-gram penalties here.\n",
    "# Call our function three times with n_gram values of 2, 3, and 4.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "def generate_with_ngram_penalty(prompt, n_gram_penalty, num_beams=6):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens,no_repeat_ngram_size = n_gram_penalty, num_beams=num_beams, pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(completion)\n",
    "    return completion\n",
    "\n",
    "generate_with_ngram_penalty(prompt,2)\n",
    "# make multiple calls to the function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      " Discipline is what you hate to do but to do it like you love it.\n",
      "\n",
      "This\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Discipline is what you hate to do but to do it like you love it.\\n\\nThis'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Same as steps 6 and 7, experiment with different settings of temperature and top_k here after instructing the model to do sampling.\n",
    "# Choose your own values for temperature and top k and see how the model's output responds.\n",
    "# Pass pad_token_id=tokenizer.eos_token_id to model.generate to prevent seeing a warning.\n",
    "\n",
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens,no_repeat_ngram_size = n_gram_penalty,temperature=temperature, top_k=top_k,do_sample = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n {completion}\")\n",
    "    return completion\n",
    "\n",
    "# make multiple calls to the function here\n",
    "generate_with_sampling(prompt,0.6,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using CodeCarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting codecarbon\n",
      "  Downloading codecarbon-2.5.0-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: arrow in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from codecarbon) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from codecarbon) (8.1.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from codecarbon) (2.2.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from codecarbon) (0.20.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\anush\\appdata\\roaming\\python\\python312\\site-packages (from codecarbon) (5.9.6)\n",
      "Collecting py-cpuinfo (from codecarbon)\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting pynvml (from codecarbon)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting rapidfuzz (from codecarbon)\n",
      "  Downloading rapidfuzz-3.9.4-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from codecarbon) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\anush\\appdata\\roaming\\python\\python312\\site-packages (from arrow->codecarbon) (2.8.2)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arrow->codecarbon) (2.9.0.20240316)\n",
      "Requirement already satisfied: colorama in c:\\users\\anush\\appdata\\roaming\\python\\python312\\site-packages (from click->codecarbon) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->codecarbon) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->codecarbon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->codecarbon) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->codecarbon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->codecarbon) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->codecarbon) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anush\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->codecarbon) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anush\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
      "Downloading codecarbon-2.5.0-py3-none-any.whl (496 kB)\n",
      "   ---------------------------------------- 0.0/496.1 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/496.1 kB 991.0 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 256.0/496.1 kB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 496.1/496.1 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.1/53.1 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.9.4-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 0.7/1.6 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 20.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 11.3 MB/s eta 0:00:00\n",
      "Installing collected packages: py-cpuinfo, rapidfuzz, pynvml, codecarbon\n",
      "Successfully installed codecarbon-2.5.0 py-cpuinfo-9.0.0 pynvml-11.5.0 rapidfuzz-3.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install codecarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anush\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.6\n",
      "Top K: 50\n",
      " Carbon dioxide is a potent greenhouse gas. Researchers at the University of Texas at Austin found that,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Carbon dioxide is a potent greenhouse gas. Researchers at the University of Texas at Austin found that,'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from codecarbon import track_emissions\n",
    "\n",
    "# 9. Add the CodeCarbon decorator to the line directly above this function.\n",
    "# Then fill the function in with your preferred settings.\n",
    "\n",
    "# add the decorator here\n",
    "def generate_with_sampling(prompt, temperature, top_k, n_gram_penalty=2):\n",
    "    tokens = encode_text_as_pt_tensor(prompt)\n",
    "    output = model.generate(tokens,no_repeat_ngram_size = n_gram_penalty,temperature=temperature, top_k=top_k,do_sample = True, pad_token_id = tokenizer.eos_token_id)\n",
    "    completion = decode_tokens(output[0])\n",
    "    print(f\"Temperature: {temperature}\\nTop K: {top_k}\\n {completion}\")\n",
    "    return completion\n",
    "\n",
    "generate_with_sampling(\"Carbon dioxide is a\", 0.6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # 10. Use pandas' read_csv method to load in the emissions.csv we generated.\n",
    "# # Then, print the first few rows with emissions.head().\n",
    "\n",
    "# df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# # 11. Pass the `emissions` DataFrame's 'duration' column to the x-axis (first argument) and the 'emissions' column to the y-axis (second argument).\n",
    "# plt.scatter # pass the columns here, then include `color='blue', alpha=0.6)`\n",
    "# plt.title('Emissions by Duration')\n",
    "# plt.xlabel('Duration (seconds)')\n",
    "# plt.ylabel('Emissions (kg CO2)')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
